{
  
    
        "post0": {
            "title": "Kaggle Titanic Dataset Top 1% Solution",
            "content": ". What are we doing? . In this blog, we use kaggle titanic dataset. Which got data of all the people who were in titanic fiasco accident. Sp, we are using some part of data to train model so that it can predict if someone survived in titanic fiasco. . Intro: . In this blog, i am explaining about how you can get kaggle top 1% score at beginner dataset. But for the better result. I am using this data. Which is a bit extension of kaggle original one. . We are not USING EDA here. This post is to show you how can a powerful tool like fastai can make life so simple. Only automatic preprocessing data in fastai and training model and getting score of top1%. . Here, we are using dataset which is a bit extension of classic titanic dataset. you can check out this: https://www.kaggle.com/pavlofesenko/titanic-extended . And It is fair to use this, i reckon. We are not altering test data at all. Only using new features(Like how we generate new features). Number of rows remains same in both test and training data. . We will be using newly released fastai2.0 library and standard sklearn. fastai2.0 is worth checking out https://docs.fast.ai/ It is more fast and efficient. With this, we can minimize our effort for preprocessing and gain better results. . Installing fastai2, yes its not preinstalled. We would need to install on our own. Incase you are running this notebook on kaggle workspace. . Warning: If you get some sort of error in importing fastai modules. I would recommend changing notebook environment to Always use latest environment Always get the latest package versions, but you may have to modify your code . !pip install fastai2 . !pip install fastcore==0.1.35 ##Currently supported with fastai2 . import fastcore fastcore.__version__ . &#39;0.1.35&#39; . import fastai2 fastai2.__version__ . &#39;0.0.30&#39; . Importing and Loading files . Now, starting with importing the intended modules. . from fastai2.tabular.all import * . df_test= pd.read_csv(&#39;/kaggle/input/titanic-extended/test.csv&#39;) df_train= pd.read_csv(&#39;../input/titanic-extended/train.csv&#39;) df_train.head() . . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare ... Embarked WikiId Name_wiki Age_wiki Hometown Boarded Destination Lifeboat Body Class . 0 1 | 0.0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | ... | S | 691.0 | Braund, Mr. Owen Harris | 22.0 | Bridgerule, Devon, England | Southampton | Qu&#39;Appelle Valley, Saskatchewan, Canada | NaN | NaN | 3.0 | . 1 2 | 1.0 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | ... | C | 90.0 | Cumings, Mrs. Florence Briggs (née Thayer) | 35.0 | New York, New York, US | Cherbourg | New York, New York, US | 4 | NaN | 1.0 | . 2 3 | 1.0 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | ... | S | 865.0 | Heikkinen, Miss Laina | 26.0 | Jyväskylä, Finland | Southampton | New York City | 14? | NaN | 3.0 | . 3 4 | 1.0 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | ... | S | 127.0 | Futrelle, Mrs. Lily May (née Peel) | 35.0 | Scituate, Massachusetts, US | Southampton | Scituate, Massachusetts, US | D | NaN | 1.0 | . 4 5 | 0.0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | ... | S | 627.0 | Allen, Mr. William Henry | 35.0 | Birmingham, West Midlands, England | Southampton | New York City | NaN | NaN | 3.0 | . 5 rows × 21 columns . Just taking abit statistical look into data. . df_train.describe() . PassengerId Survived Pclass Age SibSp Parch Fare WikiId Age_wiki Class . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | 889.000000 | 887.000000 | 889.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | 665.466817 | 29.322063 | 2.307087 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | 380.796997 | 13.930089 | 0.837713 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.420000 | 1.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | 336.000000 | 20.000000 | 2.000000 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | 672.000000 | 28.000000 | 3.000000 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | 996.000000 | 38.000000 | 3.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | 1314.000000 | 74.000000 | 3.000000 | . Before proceeding with anything else. It&#39;s important to check, if our data contains any missing values or not. For Calculating the average null values we have in our data. It is important to know. Just to get intuition about data . df_train.isnull().sum().sort_index()/len(df_train) . . Age 0.198653 Age_wiki 0.004489 Boarded 0.002245 Body 0.902357 Cabin 0.771044 Class 0.002245 Destination 0.002245 Embarked 0.002245 Fare 0.000000 Hometown 0.002245 Lifeboat 0.612795 Name 0.000000 Name_wiki 0.002245 Parch 0.000000 PassengerId 0.000000 Pclass 0.000000 Sex 0.000000 SibSp 0.000000 Survived 0.000000 Ticket 0.000000 WikiId 0.002245 dtype: float64 . Getting names of columns according to its datatype. . This will allow us to understand which feature is int, float, and object(categorical) . df_train.dtypes g_train =df_train.columns.to_series().groupby(df_train.dtypes).groups g_train . . {int64: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;], float64: [&#39;Survived&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;, &#39;Class&#39;], object: [&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Boarded&#39;, &#39;Destination&#39;, &#39;Lifeboat&#39;, &#39;Body&#39;]} . Now this might be a bit puzzling if you are new to fastai. And its completely alright. . cat_names refers to the features which are categorical. . cont_names refers to the features which are continuous. For example : int and float . fastai needs them in order to do preprocessing for you properly . cat_names= [ &#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Boarded&#39;, &#39;Destination&#39;, &#39;Lifeboat&#39;, &#39;Body&#39; ] cont_names = [ &#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;,&#39;Class&#39; ] . FastAI2 . now we are diving into fastai2. looking down at code. Might be new and intimidating. But no worries, following the documentation helped me. https://docs.fast.ai/tutorial.tabular . splits = RandomSplitter(valid_pct=0.2)(range_of(df_train)) to = TabularPandas(df_train, procs=[Categorify, FillMissing,Normalize], cat_names = cat_names, cont_names = cont_names, y_names=&#39;Survived&#39;, splits=splits) . Here we are using fastai TabularPandas library. Which will do all the preprocessing for us. before that splitting our data into validation set to have a fair amount of idea that we are not overfitting the data. valid_pct= 0.2 means (as you may have guessed by now) it means 20% validation data. . y_names, using column we are target to be predicted. procs are [Categorify, FillMissing,Normalize], Convering objects into category, filling the missing value and normalizing the content for quick processing. . Note: Tabularpandas function creates a new column if there is any missing value spotted in any particular feature(only float or int column). for example: lets say Fare has a missing value somewhere. It will create a new column named &#8217;Fare_na&#8217; where it will have integer values (1 or 2). . g_train =to.train.xs.columns.to_series().groupby(to.train.xs.dtypes).groups g_train . . {int8: [&#39;Sex&#39;, &#39;Embarked&#39;, &#39;Boarded&#39;, &#39;Lifeboat&#39;, &#39;Body&#39;, &#39;Age_na&#39;, &#39;WikiId_na&#39;, &#39;Age_wiki_na&#39;, &#39;Class_na&#39;], int16: [&#39;Name&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Destination&#39;], float64: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;, &#39;Class&#39;]} . Here you can see &#39;Age_na&#39;, &#39;WikiId_na&#39;, &#39;Age_wiki_na&#39;, &#39;Class_na&#39; . Which were created becoz their columns had missing values . Generally what happens is, we cannot use the output of tabularpandas straightaway. So we need to access it as follows.. just taking peek at train data(for validation data replace train with valid) . to.train . . PassengerId Survived Pclass Name Sex Age SibSp Parch 430 -0.059897 1.0 -1.585850 93 2 -0.120416 -0.472176 -0.462981 158 -1.123753 0.0 0.827621 768 2 -0.120416 -0.472176 -0.462981 453 0.030061 1.0 -1.585850 291 2 1.492635 0.468220 -0.462981 750 1.191698 1.0 -0.379115 853 1 -1.963903 0.468220 0.811558 250 -0.763919 0.0 0.827621 674 2 -0.120416 -0.472176 -0.462981 .. ... ... ... ... ... ... ... ... 867 1.649312 0.0 -1.585850 697 2 0.110020 -0.472176 -0.462981 193 -0.986860 1.0 -0.379115 580 2 -2.040715 0.468220 0.811558 14 -1.686971 0.0 0.827621 841 1 -1.195784 -0.472176 -0.462981 150 -1.155043 0.0 -0.379115 69 2 1.646259 -0.472176 -0.462981 580 0.526788 1.0 -0.379115 161 1 -0.350852 0.468220 0.811558 Ticket Fare ... Hometown Boarded Destination Lifeboat Body 430 4 -0.076707 ... 354 4 223 22 0 158 259 -0.525029 ... 300 4 148 0 0 453 85 1.491114 ... 291 2 149 13 0 750 235 -0.165683 ... 172 4 4 6 0 250 423 -0.560431 ... 293 4 148 0 0 .. ... ... ... ... ... ... ... ... 867 591 0.523456 ... 385 4 216 0 0 193 114 -0.090492 ... 280 4 148 22 0 14 414 -0.545288 ... 324 4 123 0 0 150 623 -0.428222 ... 194 4 107 0 23 580 134 0.009761 ... 243 4 139 4 0 Class Age_na WikiId_na Age_wiki_na Class_na 430 -1.585853 1 1 1 1 158 0.827622 2 1 1 1 453 -1.585853 1 1 1 1 750 -0.379115 1 1 1 1 250 0.827622 2 1 1 1 .. ... ... ... ... ... 867 -1.585853 1 1 1 1 193 -0.379115 1 1 1 1 14 0.827622 1 1 1 1 150 -0.379115 1 1 1 1 580 -0.379115 1 1 1 1 [713 rows x 25 columns] . using &#39;xs&#39; to get the table in the manner we are used to see it. Notice that it is without &#39;Survived&#39; column . to.train.xs . . Name Sex Ticket Cabin Embarked Name_wiki Hometown Boarded Destination Lifeboat ... Class_na PassengerId Pclass SibSp Parch Age Fare WikiId Age_wiki Class . 430 93 | 2 | 4 | 72 | 3 | 89 | 354 | 4 | 223 | 22 | ... | 1 | -0.059897 | -1.585850 | -0.472176 | -0.462981 | -0.120416 | -0.076707 | -1.694354 | -0.099986 | -1.585853 | . 158 768 | 2 | 259 | 0 | 3 | 732 | 300 | 4 | 148 | 0 | ... | 1 | -1.123753 | 0.827621 | -0.472176 | -0.462981 | -0.120416 | -0.525029 | 1.479525 | 0.551559 | 0.827622 | . 453 291 | 2 | 85 | 87 | 1 | 272 | 291 | 2 | 149 | 13 | ... | 1 | 0.030061 | -1.585850 | 0.468220 | -0.462981 | 1.492635 | 1.491114 | -1.430308 | 1.275499 | -1.585853 | . 750 853 | 1 | 235 | 0 | 3 | 818 | 172 | 4 | 4 | 6 | ... | 1 | 1.191698 | -0.379115 | 0.468220 | 0.811558 | -1.963903 | -0.165683 | -0.195430 | -1.837440 | -0.379115 | . 250 674 | 2 | 423 | 0 | 3 | 642 | 293 | 4 | 148 | 0 | ... | 1 | -0.763919 | 0.827621 | -0.472176 | -0.462981 | -0.120416 | -0.560431 | 1.268822 | -0.751531 | 0.827622 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 867 697 | 2 | 591 | 7 | 3 | 663 | 385 | 4 | 216 | 0 | ... | 1 | 1.649312 | -1.585850 | -0.472176 | -0.462981 | 0.110020 | 0.523456 | -1.147593 | 0.117196 | -1.585853 | . 193 580 | 2 | 114 | 142 | 3 | 555 | 280 | 4 | 148 | 22 | ... | 1 | -0.986860 | -0.379115 | 0.468220 | 0.811558 | -2.040715 | -0.090492 | -0.387463 | -1.909834 | -0.379115 | . 14 841 | 1 | 414 | 0 | 3 | 806 | 324 | 4 | 123 | 0 | ... | 1 | -1.686971 | 0.827621 | -0.472176 | -0.462981 | -1.195784 | -0.545288 | 1.650221 | -1.113501 | 0.827622 | . 150 69 | 2 | 623 | 0 | 3 | 69 | 194 | 4 | 107 | 0 | ... | 1 | -1.155043 | -0.379115 | -0.472176 | -0.462981 | 1.646259 | -0.428222 | -0.878214 | 1.565074 | -0.379115 | . 580 161 | 1 | 134 | 0 | 3 | 151 | 243 | 4 | 139 | 4 | ... | 1 | 0.526788 | -0.379115 | 0.468220 | 0.811558 | -0.350852 | 0.009761 | -0.776863 | -0.317168 | -0.379115 | . 713 rows × 24 columns . using the same for target value . to.train.ys.values.ravel() . . array([1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.], dtype=float32) . Model: . Now, we got our data preprocessed. It&#39;s time to model the data. For this problem i am using RandomForestClassifier it&#39;s powerful and simple. And taking out X_train and y_train values like we showed before examples. . from sklearn.ensemble import RandomForestClassifier X_train, y_train = to.train.xs, to.train.ys.values.ravel() X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel() . We have table without any hardcore preprocessing all we did was just to use fastai tabular function to get this . X_train.head() . Name Sex Ticket Cabin Embarked Name_wiki Hometown Boarded Destination Lifeboat ... Class_na PassengerId Pclass SibSp Parch Age Fare WikiId Age_wiki Class . 430 93 | 2 | 4 | 72 | 3 | 89 | 354 | 4 | 223 | 22 | ... | 1 | -0.059897 | -1.585850 | -0.472176 | -0.462981 | -0.120416 | -0.076707 | -1.694354 | -0.099986 | -1.585853 | . 158 768 | 2 | 259 | 0 | 3 | 732 | 300 | 4 | 148 | 0 | ... | 1 | -1.123753 | 0.827621 | -0.472176 | -0.462981 | -0.120416 | -0.525029 | 1.479525 | 0.551559 | 0.827622 | . 453 291 | 2 | 85 | 87 | 1 | 272 | 291 | 2 | 149 | 13 | ... | 1 | 0.030061 | -1.585850 | 0.468220 | -0.462981 | 1.492635 | 1.491114 | -1.430308 | 1.275499 | -1.585853 | . 750 853 | 1 | 235 | 0 | 3 | 818 | 172 | 4 | 4 | 6 | ... | 1 | 1.191698 | -0.379115 | 0.468220 | 0.811558 | -1.963903 | -0.165683 | -0.195430 | -1.837440 | -0.379115 | . 250 674 | 2 | 423 | 0 | 3 | 642 | 293 | 4 | 148 | 0 | ... | 1 | -0.763919 | 0.827621 | -0.472176 | -0.462981 | -0.120416 | -0.560431 | 1.268822 | -0.751531 | 0.827622 | . 5 rows × 24 columns . Model initialization using sklearn. We are simply using 100 different decision trees. Keeping the model parameters simple as possible so that we don&#39;t overfit either. . rnf_classifier= RandomForestClassifier(n_estimators=100, n_jobs=-1) rnf_classifier.fit(X_train,y_train) . RandomForestClassifier(n_jobs=-1) . We just Trained randomforest classifier and predicting accuracy on validation set . Accuracy on validation dataset . y_pred=rnf_classifier.predict(X_valid) from sklearn.metrics import accuracy_score accuracy_score(y_pred, y_valid) . 0.9943820224719101 . WOW, we get the accuracy of 0.99 on validation dataset this means on leader board getting rank of 200 something our of 19,000. . TEST dataset . We need to turn the test data format as intended so that to out feed in model. Taking look at test data . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked WikiId Name_wiki Age_wiki Hometown Boarded Destination Lifeboat Body Class . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | 928.0 | Kelly, Mr. James | 19.0 | Unknown, Ireland | Southampton | New York City | NaN | 70MB | 3.0 | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | 1297.0 | Wilkes, Mrs. Ellen | 47.0 | Penzance, Cornwall, England | Southampton | Akron, Ohio, US | 16 | NaN | 3.0 | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | 518.0 | Myles, Mr. Thomas Francis | 63.0 | Fermoy, Ireland[note 1] | Queenstown | Waban, Massachusetts, US | NaN | NaN | 2.0 | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | 1303.0 | Wirz, Mr. Albert | 27.0 | Uster, Switzerland | Southampton | Beloit, Wisconsin, US | NaN | 131MB | 3.0 | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | 871.0 | Hirvonen, Mrs. Helga Elisabeth (née Lindqvist) | 22.0 | Taalintehdas, Finland | Southampton | Monessen, Pennsylvania, US | 15 | NaN | 3.0 | . doing the same preprocessing we did for training set . df_test.dtypes g_train =df_test.columns.to_series().groupby(df_test.dtypes).groups g_train . {int64: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;], float64: [&#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;, &#39;Class&#39;], object: [&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Boarded&#39;, &#39;Destination&#39;, &#39;Lifeboat&#39;, &#39;Body&#39;]} . cat_names= [ &#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Boarded&#39;, &#39;Destination&#39;, &#39;Lifeboat&#39;, &#39;Body&#39; ] cont_names = [ &#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;,&#39;Class&#39; ] . . Now further, Avoiding y_name and split for obvious reasons. . test = TabularPandas(df_test, procs=[Categorify, FillMissing,Normalize], cat_names = cat_names, cont_names = cont_names, ) . . getting the table value for test dataset. we used train.xs becoz it lets us extact the data we had given the tabularpandas function . X_test.head() . . Name Sex Ticket Cabin Embarked Name_wiki Hometown Boarded Destination Lifeboat ... Class_na PassengerId Pclass SibSp Parch Age Fare WikiId Age_wiki Class . 0 207 | 2 | 153 | 0 | 2 | 194 | 226 | 4 | 103 | 0 | ... | 1 | -1.727912 | 0.873482 | -0.499470 | -0.400248 | 0.386231 | -0.497413 | 0.752828 | -0.794545 | 0.871140 | . 1 404 | 1 | 222 | 0 | 3 | 389 | 162 | 4 | 2 | 9 | ... | 1 | -1.719625 | 0.873482 | 0.616992 | -0.400248 | 1.371369 | -0.512278 | 1.729823 | 1.304755 | 0.871140 | . 2 270 | 2 | 74 | 0 | 2 | 257 | 65 | 3 | 154 | 0 | ... | 1 | -1.711337 | -0.315819 | -0.499470 | -0.400248 | 2.553536 | -0.464100 | -0.332721 | 2.504355 | -0.311121 | . 3 409 | 2 | 148 | 0 | 3 | 393 | 233 | 4 | 13 | 0 | ... | 1 | -1.703050 | 0.873482 | -0.499470 | -0.400248 | -0.204852 | -0.482475 | 1.745709 | -0.194745 | 0.871140 | . 4 179 | 1 | 139 | 0 | 3 | 167 | 208 | 4 | 94 | 7 | ... | 1 | -1.694763 | 0.873482 | 0.616992 | 0.619896 | -0.598908 | -0.417491 | 0.601910 | -0.569620 | 0.871140 | . 5 rows × 25 columns . Now taking a look at features present in test data. . X_test.dtypes g_train =X_test.columns.to_series().groupby(X_test.dtypes).groups g_train . . {int8: [&#39;Sex&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;Boarded&#39;, &#39;Lifeboat&#39;, &#39;Body&#39;, &#39;Age_na&#39;, &#39;Fare_na&#39;, &#39;WikiId_na&#39;, &#39;Age_wiki_na&#39;, &#39;Class_na&#39;], int16: [&#39;Name&#39;, &#39;Ticket&#39;, &#39;Name_wiki&#39;, &#39;Hometown&#39;, &#39;Destination&#39;], float64: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;WikiId&#39;, &#39;Age_wiki&#39;, &#39;Class&#39;]} . . Warning: See that we are dropping Fare_na column . X_test= X_test.drop(&#39;Fare_na&#39;, axis=1) . Now you would be thinking why we are dropping the &#39;Fare_na&#39; column. The reason is, originally we trained model on 24 features. But as you know that tabularpandas function creates a new column if there is any missing value spotted in any particular feature. . Here, our test data had some Fare values missing so the function created the boolean column this lead to 25 features. If you try to pass without dropping Fare_na column, it will give you the error that model was trained on 24 features and we are passing 25. That&#39;s why dropping Fare_na feature. And dont worry it doesnt have any effect on the output . Here is our output . output= pd.DataFrame({&#39;PassengerId&#39;:df_test.PassengerId, &#39;Survived&#39;: y_pred}) output.to_csv(&#39;my_submission_titanic.csv&#39;, index=False) output.head() . PassengerId Survived . 0 892 | 0 | . 1 893 | 1 | . 2 894 | 0 | . 3 895 | 0 | . 4 896 | 1 | .",
            "url": "https://hiteshhedwig.github.io/hedwig-explains/fastai/kaggle/datascience/2020/10/13/titanic-fastai.html",
            "relUrl": "/fastai/kaggle/datascience/2020/10/13/titanic-fastai.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Biggest Dataset on Internet",
            "content": ". What are we doing? And Why? . If you are a datascientist or computer vision researcher who always looking for neat image or any sort of dataset because it&#39;s sometimes so hard to find right dataset for you. In this post, i am sharing some resources which would be super helpful for you. I will also show the right way to download them in case of Google Open Image Dataset. What right way you may ask? It&#39;s simply a python script which will do the job for you. You don&#39;t need to rush in anyway. . Open Images Dataset V6 . When i was starting out from scratch. It was quite difficult to know from where to download the dataset. Or if that dataset right for you. Open Images Dataset V6 by Google is an amazing source to download the data. . You can find it here. There will be plethora of categories in the dropdown menu. It would look something like this. . . Brief . As you can see, the category for this tutorial i have chosen is taxi. You may chose anything else, Ofcourse!. There are several filters on the top of red bar in the website which is important to know about. Like: . Subset : (Train, Validation) | Type: (Detection, Segementation) | . Subset is only to show you the content which will be downloaded if you download train or validation filtered data. . Type is crucial, it will give you whatever type of problem deal with. For example, for this example we have used detection. So the images we are getting is bounding boxes. If you switch it to, segmentation you get segemented image. As simple as that. . How to download? . It&#39;s quite difficult to ambigous to download from the website. But fortunately we have tool which makes it easy to one liner! . We use a tool name OIDv4_ToolKit available on github. It makes is fairly easy to download images. . Cloning the github repo. . Note: If you are running the command in a terminal. Omit &quot;!&quot; . . !git clone https://github.com/theAIGuysCode/OIDv4_ToolKit.git . Cloning into &#39;OIDv4_ToolKit&#39;... remote: Enumerating objects: 444, done. remote: Total 444 (delta 0), reused 0 (delta 0), pack-reused 444 Receiving objects: 100% (444/444), 34.09 MiB | 35.95 MiB/s, done. Resolving deltas: 100% (157/157), done. . Moving inside directory and extracting some files. You don&#39;t need to bother much about this, just copy paste and run on your machine. . %cd OIDv4_ToolKit/ !curl &quot;https://d1vvhvl2y92vvt.cloudfront.net/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; !unzip awscliv2.zip !sudo ./aws/install . Now, here comes the magic. One command where you specify : . class : In our case, we will download Taxi images. You can download multiple classes by just typing class one after another. | type_csv: Do you want to download train data? Validation data? | limit: How many images we want to download? I am downloading 100 as an example. | . !python main.py downloader --classes Taxi --type_csv train --limit 100 . ___ _____ ______ _ _ .&#39; `.|_ _||_ _ `. | | | | / .-. | | | | `. _ __ | |__| |_ | | | | | | | | | |[ [ ]|____ _| `-&#39; /_| |_ _| |_.&#39; / / / _| |_ `.___.&#39;|_____||______.&#39; __/ |_____| _____ _ _ (____ | | | | _ ___ _ _ _ ____ | | ___ ____ _ | | ____ ____ | | | / _ | | | | _ | |/ _ / _ |/ || |/ _ )/ ___) | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| | |_____/ ___/ ____|_| |_|_| ___/ _||_| ____| ____)_| [INFO] | Downloading Taxi. [ERROR] | Missing the class-descriptions-boxable.csv file. [DOWNLOAD] | Do you want to download the missing file? [Y/n] Y ...145%, 0 MB, 31097 KB/s, 0 seconds passed [DOWNLOAD] | File class-descriptions-boxable.csv downloaded into OID/csv_folder/class-descriptions-boxable.csv. [ERROR] | Missing the train-annotations-bbox.csv file. [DOWNLOAD] | Do you want to download the missing file? [Y/n] Y ...100%, 1138 MB, 33727 KB/s, 34 seconds passed [DOWNLOAD] | File train-annotations-bbox.csv downloaded into OID/csv_folder/train-annotations-bbox.csv. Taxi [INFO] | Downloading train images. [INFO] | [INFO] Found 1434 online images for train. [INFO] | Limiting to 100 images. [INFO] | Download of 100 images in train. 100% 100/100 [02:08&lt;00:00, 1.28s/it] [INFO] | Done! [INFO] | Creating labels for Taxi of train. [INFO] | Labels creation completed. . Sample image from data . Now it has been downloaded. Dataset will be downloaded in the file /OID/Dataset/train/ Let&#39;s see the sample image? . So it looks pretty good! Remember i have only downloaded 100 images. You can download with any limit. If its available on dataset. It will be downloaded. We also have bounding boxes in labels folder. . . Tip: csv file have been downloaded in csv_folder. You can use it as pandas dataframe for more flexible usage of data. . Open Public datasets . As a data scientist, you dont always deal with image dataset. So, this Github Repo got very detailed list of every dataset for gamut of professions. . . . Tip: If you are new to github. You can fork it and contribute to it as well. . Amazon, Google, Microsoft Public Dataset . Waait.. we just discussed google dataset a while ago. That was especially for image based problems. Incase you want to research for the data yourself that you struggling to find. The Google Dataset Search engine will help you to research more about it. . Like Google, Amazon also have some public dataset to help you with your research. . And so do, Microsoft . . Datasets we have discussed so far. They will definitely provide the edge you looking for (if you look correctly). They are almost all you need. Although there are sites like kaggle, datatruks but as we have mentioned google dataset engine. It automatically directs you to the sites. .",
            "url": "https://hiteshhedwig.github.io/hedwig-explains/dataset/opensource/2020/10/07/opensource-dataset.html",
            "relUrl": "/dataset/opensource/2020/10/07/opensource-dataset.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Dog Breed Classification:",
            "content": "Intro: . We have used famous standford dataset. The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. . I have downloaded it manually and uploaded to my google drive. Later made a webapp. . Fastai2: It&#39;s very famous A.I library built on the top of pytorch. Courses taught by himself Jeremey Howard here. Do go through it, it might look a bit long but it&#39;s worth it! . Before proceeding further, if you are a beginner, you should know that google colab comes with preinstalled libraries. So, we rarely have to install something on our own. But for this task, we will be using fastai2. And Installing it because its not pre-installed. . Gathering data and using: . Here, we are connecting our google colab notebook to google drive. So that, the data i have downloaded can be loaded and used for training. When you will download data the folder names i.e classes would be filled with numbers and special characters. But we want simple class name like, Pembroke not n02113023-Pembroke . So, a bit cleaning was necessary. In fastai, Jeremy Howard talks about using regular expression technique which is indeed quite handy. . But i haven&#39;t used it, i used simple custom function to change the names. Which worked pretty well! . You can find that blog here in this link. It&#39;s small and quick. . from google.colab import drive drive.mount(&quot;/content/gdrive&quot;) . Mounted at /content/gdrive . . Note: that at the time of running this notebook. Fastai2 was supported by fastcore 0.1.35 somehow. . !pip install fastai2 !pip install fastcore==0.1.35 . . Important: In standard software engineering practices, it is not recommended to use from {some package} import *. It is said to pollute namespace and make code less readable and sometimes prone to breakage. . from fastai2.vision.all import * . Then why are we using it? . According to the creator, Jeremy Howard, it&#39;s recommended way in fastai2. Fastai2 is designed in a way to make from fastai2.vision.all import * efficient and easy to use. It doesn&#39;t causes namespace pollution and keeps code clean. To learn about it more you can refer to fastai official docs. . Now, loading data from our dataset directory. Fastai provides pretty easy way to deal with it. . dls = ImageDataLoaders.from_folder(path,train=&#39;train&#39;, valid=&#39;val&#39;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224,flip_vert=True), device=&#39;cuda&#39;) . In fastai, if you are getting started. It might look a bit overwhelming to get familiar with words. Just like me, perhaps. But bear with it. It&#39;s worth it. . Now what is ImageDataLoaders.from_folder? Fastai has various dataloaders to take use data and convert them into batches and other type of augmentation. Look into this. These functions basically load your data and convert them in batches. Either, loading from folder, df, path func.. etc. We are using ImageDataLoaders.from_folder because we have image data saved in it. . Parameters of this functions like, path, train, val, item_tfms, batch_tfms. In first, we define path to the directory. Within the directory i have image folder named train and val. Giving these folders name to function so that it can go on finding the data. . item_tfms happens first, followed by batch_tfms. This enables most of the calculations for the transforms to happen on the GPU, thus saving time. The first step item_tfms resizes all the images to the same size (this happens on the CPU) and then batch_tfms happens on the GPU for the entire batch of images. If it weren’t to happen in two steps, then all these calculations would have to be done on the CPU, which is slower. . . As you can see, item_tfms uses CPU for resizing images to same size. So that, our model generalizes well. And then we use batch_tfms to push everything to GPU. . Now after data preprocessing. Shall we look into our data? How it looks. It&#39;s important to be familiar with data and get comfortable with it. . dls.show_batch() . . Modelling in Fastai2 . We use cnn_learner to built our model in single line. Although so many is happening under the hood. Remember that we aare passing several things like, dls which is our dataloader we created a while ago. resnet50 is our pretrained state of the art model. Incase, you are absolute beginner. We use technique called transfer learning. . What it does? It take pretrained. State of the art model. And fine tune it. It means, existing pretrained model and change several parameters according to our needs and MAGIC we get our own state of the art model. . This is what cnn_learner does. We give it data, and tells which model to use. We could also use resnet18,34,101. But in this case, resnet 50 worked pretty well. And we also pass metric to be used, remember metric is important parameter as it tells us our model is not overfitting. We used both error_rate and accuracy. In many cases, error_rate is your best friend. . learn = cnn_learner(dls, resnet50, metrics=[accuracy,error_rate]) . Before going to the training we should find learning rate at which model will be learning the data. . Warning: Don&#8217;t fall in trap of choosing high learning rate so that our model learns fastest. . Why? Because faster learning means model missing out the underlying relation in the data. Rather struggles and won&#39;t give you satisfactory result. Neither, we should choose low learning rate coz it might take eternity to reach to solution. We want abstemious learning rate so that it fits well. . . So, how to choose learning rate? Fastai to the rescue! . Below, an image will help you. Choosing learning rate. . learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.009120108559727669) . Oops, It is some curvy graph. And we are provided with 2 suggested LR(Learning Rate) . How to choose best learning rate? You might hypothesis that learning rate which gives lowest would be ideal. . Not quite. We want to select learning rate point at which it is dropping fastest per step. This happening around near between 10^-3 and 10^-2. We can choose, 1e-3. Which in normal notations is 0.0010. . Model Training And Result . Well, now the time to have fun! Let&#39;s see how our model performs. I hope not too bad. We use fine_tune function to set number of epochs and learning rate. Let&#39;s set number of epochs to 4. . learn.fine_tune(4, 1e-3) . epoch train_loss valid_loss accuracy error_rate time . 0 | 1.477997 | 0.959091 | 0.719900 | 0.280100 | 02:48 | . epoch train_loss valid_loss accuracy error_rate time . 0 | 1.112022 | 0.758686 | 0.771144 | 0.228856 | 02:54 | . 1 | 0.913335 | 0.653243 | 0.800995 | 0.199005 | 02:55 | . 2 | 0.741961 | 0.605838 | 0.815920 | 0.184080 | 02:55 | . 3 | 0.636256 | 0.594278 | 0.828856 | 0.171144 | 02:57 | . Ummm, 82% accuracy. Not exceptional not bad either. Actually, it works very fine. You can do better! Try yourself and see if you can do better than this. . Let&#39;s see results below . learn.show_results() . . So our model is detecting pretty nicely?!!! Just one error. With so minimal effort we trained state of the art model and it is working pretty well. If you can train nicely, it will be flawless! . But, let&#39;s just look at where we are wrong. Where our model is making mistakes? . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(15,10)) . . We can also plot confusion matrix. It helps us to see if the predictions are right or not. Below image shows some white diagonal. It means most of the images were classified correctly. If you want to learn more about confusion matrix read here. . k= ClassificationInterpretation.from_learner(learn) k.plot_confusion_matrix() . Let&#39;s just predict on any random image. I had test dataset which i didn&#39;t use in my training. So, using any one image. I chose, Beagle image. Let&#39;s see if our model can predict correctly. . . Tip: You can use softmax function to show the probability of prediction. . breed=learn.predict(img) . . &#34;Dog&#39;s Breed is Beagle&#34; . Exporting Model and Loading it . One of the questions could be how to export model? We use fairly easy way in exporting and loading models in fastai2. . learn.export(&#39;final.pth&#39;) #exporting . learn= load_learner(&#39;final.pth&#39;) #loading . Again using prediction to see if our model is still able to recognize Beagle image we used a while ago . &#34;Dog&#39;s Breed is Beagle&#34; . So, its working pretty nicely! . If it helped you or you have any queries feel free to ask! .",
            "url": "https://hiteshhedwig.github.io/hedwig-explains/python/deeplearning/classification/fastai/2020/10/05/dog-breed-fastai.html",
            "relUrl": "/python/deeplearning/classification/fastai/2020/10/05/dog-breed-fastai.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Changing Folder Name for model training",
            "content": "What are we doing in this blog? . We are changing folders name like this: . Folder Name from n02113023-Pembroke Changed to Pembroke . Folder Name from n02113624-toy_poodle Changed to Toy Poodle . Folder Name from n02115641-dingo Changed to Dingo . If want to learn this, Go on with reading: . In this, dataset directory. I have downloaded, famous standford dataset. The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. . The name of the folders (which will be used as classes while training) have names like, n02093256-Staffordshire_bullterrier. We definitely don&#39;t want our class to be like that but rather simple Staffordshire Bullterrier. The python script will get rid of any weird characters that are unneccessary. . Now, we have our path defined where the data is situated. In my case, i had uploaded it to google drive. Time consuming upload, avoid it if possible. . path= &#39;/content/gdrive/My Drive/Dog species/Images&#39; . . Let&#39;s say we have folder name like this: . n02093256-Staffordshire_bullterrier . Here is our process of obtaining what we want, . We first split on the basis of - (hyphen) charachter | Then we obtain list like this: [&#39;n02093256&#39; , &#39;Staffordshire_bullterrier&#39;] | After that, we use slicing technique to select second index. | Then after we have what we wanted, we can simply again split on the basis of _ (underscore) | Then we obtain something like this: [&#39;Staffordshire&#39;,&#39;bullterrier&#39;] | After we have what we wanted we can simply join the list back. And use capitalize to make first letter of word capitalize. | . . Note: This code will need to be modified according to the folder name you are dealing with. . &#39;&#39;&#39; THIS PYTHON CODE CHANGES NAME OF FILES IN A FOLDER &#39;&#39;&#39; import os import string def change_name(folder_name): folder_name=folder_name.split(&#39;-&#39;) #n02113023-Pembroke-&gt;[&#39;n02113023&#39;,&#39;Pembroke&#39;] folder_name= &#39; &#39;.join(folder_name[1:]) #Selecting index after 1 and joining them folder_name=folder_name.split(&#39;_&#39;) # Again splitting becayse name seem to be like Staffordshire_bullterrier folder_name= &#39; &#39;.join(folder_name) #again joining after getting rid of _ return string.capwords(folder_name) #or we can use capitalize() #iterating over all the folders and changing file name for fn in os.listdir(path): new_path= os.path.join(path,fn) #folder_name= os.path.basename(new_path) #alternative use of to know file name new_folder_name= change_name(fn) os.rename(os.path.join(path,fn),os.path.join(path,new_folder_name)) print (f&#39;Folder Name from {fn} Changed to {new_folder_name}&#39;) . Folder Name from n02113023-Pembroke Changed to Pembroke Folder Name from n02093256-Staffordshire_bullterrier Changed to Staffordshire Bullterrier Folder Name from n02093428-American_Staffordshire_terrier Changed to American Staffordshire Terrier Folder Name from n02113624-toy_poodle Changed to Toy Poodle Folder Name from n02100236-German_short-haired_pointer Changed to German Short Haired Pointer Folder Name from n02115641-dingo Changed to Dingo Folder Name from n02089867-Walker_hound Changed to Walker Hound Folder Name from n02099601-golden_retriever Changed to Golden Retriever Folder Name from n02105162-malinois Changed to Malinois Folder Name from n02100735-English_setter Changed to English Setter Folder Name from n02097298-Scotch_terrier Changed to Scotch Terrier Folder Name from n02095889-Sealyham_terrier Changed to Sealyham Terrier Folder Name from n02106550-Rottweiler Changed to Rottweiler Folder Name from n02088094-Afghan_hound Changed to Afghan Hound Folder Name from n02112018-Pomeranian Changed to Pomeranian Folder Name from n02099429-curly-coated_retriever Changed to Curly Coated Retriever Folder Name from n02095314-wire-haired_fox_terrier Changed to Wire Haired Fox Terrier Folder Name from n02116738-African_hunting_dog Changed to African Hunting Dog Folder Name from n02091467-Norwegian_elkhound Changed to Norwegian Elkhound Folder Name from n02096294-Australian_terrier Changed to Australian Terrier Folder Name from n02108422-bull_mastiff Changed to Bull Mastiff Folder Name from n02096177-cairn Changed to Cairn Folder Name from n02104365-schipperke Changed to Schipperke Folder Name from n02101556-clumber Changed to Clumber Folder Name from n02090721-Irish_wolfhound Changed to Irish Wolfhound Folder Name from n02110806-basenji Changed to Basenji Folder Name from n02105251-briard Changed to Briard Folder Name from n02102040-English_springer Changed to English Springer Folder Name from n02085620-Chihuahua Changed to Chihuahua Folder Name from n02110063-malamute Changed to Malamute Folder Name from n02109525-Saint_Bernard Changed to Saint Bernard Folder Name from n02107683-Bernese_mountain_dog Changed to Bernese Mountain Dog Folder Name from n02111129-Leonberg Changed to Leonberg Folder Name from n02094114-Norfolk_terrier Changed to Norfolk Terrier Folder Name from n02110627-affenpinscher Changed to Affenpinscher Folder Name from n02111277-Newfoundland Changed to Newfoundland Folder Name from n02112350-keeshond Changed to Keeshond Folder Name from n02106382-Bouvier_des_Flandres Changed to Bouvier Des Flandres Folder Name from n02093647-Bedlington_terrier Changed to Bedlington Terrier Folder Name from n02107312-miniature_pinscher Changed to Miniature Pinscher Folder Name from n02115913-dhole Changed to Dhole Folder Name from n02111889-Samoyed Changed to Samoyed Folder Name from n02091032-Italian_greyhound Changed to Italian Greyhound Folder Name from n02085782-Japanese_spaniel Changed to Japanese Spaniel Folder Name from n02098286-West_Highland_white_terrier Changed to West Highland White Terrier Folder Name from n02090379-redbone Changed to Redbone Folder Name from n02099849-Chesapeake_Bay_retriever Changed to Chesapeake Bay Retriever Folder Name from n02106662-German_shepherd Changed to German Shepherd Folder Name from n02105505-komondor Changed to Komondor Folder Name from n02087046-toy_terrier Changed to Toy Terrier Folder Name from n02098105-soft-coated_wheaten_terrier Changed to Soft Coated Wheaten Terrier Folder Name from n02099267-flat-coated_retriever Changed to Flat Coated Retriever Folder Name from n02104029-kuvasz Changed to Kuvasz Folder Name from n02096585-Boston_bull Changed to Boston Bull Folder Name from n02097130-giant_schnauzer Changed to Giant Schnauzer Folder Name from n02086646-Blenheim_spaniel Changed to Blenheim Spaniel Folder Name from n02112706-Brabancon_griffon Changed to Brabancon Griffon Folder Name from n02111500-Great_Pyrenees Changed to Great Pyrenees Folder Name from n02088466-bloodhound Changed to Bloodhound Folder Name from n02101006-Gordon_setter Changed to Gordon Setter Folder Name from n02108089-boxer Changed to Boxer Folder Name from n02113799-standard_poodle Changed to Standard Poodle Folder Name from n02086910-papillon Changed to Papillon Folder Name from n02113712-miniature_poodle Changed to Miniature Poodle Folder Name from n02095570-Lakeland_terrier Changed to Lakeland Terrier Folder Name from n02098413-Lhasa Changed to Lhasa Folder Name from n02106030-collie Changed to Collie Folder Name from n02092002-Scottish_deerhound Changed to Scottish Deerhound Folder Name from n02110185-Siberian_husky Changed to Siberian Husky Folder Name from n02088238-basset Changed to Basset Folder Name from n02097047-miniature_schnauzer Changed to Miniature Schnauzer Folder Name from n02108551-Tibetan_mastiff Changed to Tibetan Mastiff Folder Name from n02105412-kelpie Changed to Kelpie Folder Name from n02106166-Border_collie Changed to Border Collie Folder Name from n02102480-Sussex_spaniel Changed to Sussex Spaniel Folder Name from n02110958-pug Changed to Pug Folder Name from n02109961-Eskimo_dog Changed to Eskimo Dog Folder Name from n02096437-Dandie_Dinmont Changed to Dandie Dinmont Folder Name from n02091831-Saluki Changed to Saluki Folder Name from n02105056-groenendael Changed to Groenendael Folder Name from n02113186-Cardigan Changed to Cardigan Folder Name from n02102973-Irish_water_spaniel Changed to Irish Water Spaniel Folder Name from n02113978-Mexican_hairless Changed to Mexican Hairless Folder Name from n02092339-Weimaraner Changed to Weimaraner Folder Name from n02105855-Shetland_sheepdog Changed to Shetland Sheepdog Folder Name from n02089973-English_foxhound Changed to English Foxhound Folder Name from n02102318-cocker_spaniel Changed to Cocker Spaniel Folder Name from n02097658-silky_terrier Changed to Silky Terrier Folder Name from n02088632-bluetick Changed to Bluetick Folder Name from n02091635-otterhound Changed to Otterhound Folder Name from n02108000-EntleBucher Changed to Entlebucher Folder Name from n02094258-Norwich_terrier Changed to Norwich Terrier Folder Name from n02112137-chow Changed to Chow Folder Name from n02094433-Yorkshire_terrier Changed to Yorkshire Terrier Folder Name from n02097474-Tibetan_terrier Changed to Tibetan Terrier Folder Name from n02100583-vizsla Changed to Vizsla Folder Name from n02097209-standard_schnauzer Changed to Standard Schnauzer Folder Name from n02096051-Airedale Changed to Airedale Folder Name from n02091134-whippet Changed to Whippet Folder Name from n02107908-Appenzeller Changed to Appenzeller Folder Name from n02105641-Old_English_sheepdog Changed to Old English Sheepdog Folder Name from n02085936-Maltese_dog Changed to Maltese Dog Folder Name from n02087394-Rhodesian_ridgeback Changed to Rhodesian Ridgeback Folder Name from n02108915-French_bulldog Changed to French Bulldog Folder Name from n02100877-Irish_setter Changed to Irish Setter Folder Name from n02109047-Great_Dane Changed to Great Dane Folder Name from n02107574-Greater_Swiss_Mountain_dog Changed to Greater Swiss Mountain Dog Folder Name from n02086240-Shih-Tzu Changed to Shih Tzu Folder Name from n02093859-Kerry_blue_terrier Changed to Kerry Blue Terrier Folder Name from n02086079-Pekinese Changed to Pekinese Folder Name from n02107142-Doberman Changed to Doberman Folder Name from n02088364-beagle Changed to Beagle Folder Name from n02101388-Brittany_spaniel Changed to Brittany Spaniel Folder Name from n02093754-Border_terrier Changed to Border Terrier Folder Name from n02089078-black-and-tan_coonhound Changed to Black And Tan Coonhound Folder Name from n02099712-Labrador_retriever Changed to Labrador Retriever Folder Name from n02093991-Irish_terrier Changed to Irish Terrier Folder Name from n02090622-borzoi Changed to Borzoi Folder Name from n02091244-Ibizan_hound Changed to Ibizan Hound Folder Name from n02102177-Welsh_springer_spaniel Changed to Welsh Springer Spaniel . count=0 for fn in os.listdir(path): count+=1 print(fn) print(count) . Pembroke Staffordshire Bullterrier American Staffordshire Terrier Toy Poodle German Short Haired Pointer Dingo Walker Hound Golden Retriever Malinois English Setter Scotch Terrier Sealyham Terrier Rottweiler Afghan Hound Pomeranian Curly Coated Retriever Wire Haired Fox Terrier African Hunting Dog Norwegian Elkhound Australian Terrier Bull Mastiff Cairn Schipperke Clumber Irish Wolfhound Basenji Briard English Springer Chihuahua Malamute Saint Bernard Bernese Mountain Dog Leonberg Norfolk Terrier Affenpinscher Newfoundland Keeshond Bouvier Des Flandres Bedlington Terrier Miniature Pinscher Dhole Samoyed Italian Greyhound Japanese Spaniel West Highland White Terrier Redbone Chesapeake Bay Retriever German Shepherd Komondor Toy Terrier Soft Coated Wheaten Terrier Flat Coated Retriever Kuvasz Boston Bull Giant Schnauzer Blenheim Spaniel Brabancon Griffon Great Pyrenees Bloodhound Gordon Setter Boxer Standard Poodle Papillon Miniature Poodle Lakeland Terrier Lhasa Collie Scottish Deerhound Siberian Husky Basset Miniature Schnauzer Tibetan Mastiff Kelpie Border Collie Sussex Spaniel Pug Eskimo Dog Dandie Dinmont Saluki Groenendael Cardigan Irish Water Spaniel Mexican Hairless Weimaraner Shetland Sheepdog English Foxhound Cocker Spaniel Silky Terrier Bluetick Otterhound Entlebucher Norwich Terrier Chow Yorkshire Terrier Tibetan Terrier Vizsla Standard Schnauzer Airedale Whippet Appenzeller Old English Sheepdog Maltese Dog Rhodesian Ridgeback French Bulldog Irish Setter Great Dane Greater Swiss Mountain Dog Shih Tzu Kerry Blue Terrier Pekinese Doberman Beagle Brittany Spaniel Border Terrier Black And Tan Coonhound Labrador Retriever Irish Terrier Borzoi Ibizan Hound Welsh Springer Spaniel 120 . So we did what we intended to. Removing weird characters and keeping relevent ones. .",
            "url": "https://hiteshhedwig.github.io/hedwig-explains/python/filechange/datascience/2020/10/04/changing-folders-name.html",
            "relUrl": "/python/filechange/datascience/2020/10/04/changing-folders-name.html",
            "date": " • Oct 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hiteshhedwig.github.io/hedwig-explains/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is a blogging website where i share things both technical and spiritual ideas under the penname of hedwig. Connect with me : . Linkedin . Website .",
          "url": "https://hiteshhedwig.github.io/hedwig-explains/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hiteshhedwig.github.io/hedwig-explains/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}